{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choi/Downloads/miniconda3/envs/test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import (\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel,\n",
    "    # UNet2DModel,\n",
    "    AutoencoderKL,\n",
    "    StableDiffusionPipeline,\n",
    "    DiffusionPipeline,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import convert_state_dict_to_diffusers\n",
    "from diffusers.training_utils import cast_training_params\n",
    "from diffusers.utils import make_image_grid\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from peft import LoraConfig\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"output_dir\": \"./output\",\n",
    "    # \"log_dir\": \"./log\",\n",
    "    \"batch_size\": 8,\n",
    "    \"train_epochs\": 20,\n",
    "    \"save_ckpt_every_n_epochs\": 5,\n",
    "    \"validate_every_n_epochs\": 2,\n",
    "    \"lora_rank\": 4,\n",
    "    # \"lora_rank\": 2,\n",
    "    # \"lora_rank\": 8,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# accelerator_project_config = ProjectConfiguration(project_dir=config[\"output_dir\"], logging_dir=config[\"log_dir\"])\n",
    "accelerator_project_config = ProjectConfiguration(project_dir=config[\"output_dir\"])\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "# os.makedirs(config[\"log_dir\"], exist_ok=True)\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"fp16\",  # use amp\n",
    "    project_config=accelerator_project_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading and loading a dataset from the hub.\n",
    "dataset = load_dataset(\n",
    "    \"kusnim1121/filtered-one-piece-with-caption\",\n",
    ")\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.RandomCrop(512),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    captions = tokenizer(\n",
    "        examples[\"caption\"],\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "    return {\"images\": images, \"captions\": captions}\n",
    "\n",
    "\n",
    "# Set the training transforms\n",
    "train_dataset = dataset[\"train\"].with_transform(transform)\n",
    "# val_dataset = dataset[\"val\"].with_transform(transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD setup & LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scheduler, tokenizer and models.\n",
    "# SD_PATH = \"/choi/model/stable-diffusion-v1-5\"\n",
    "SD_PATH = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(SD_PATH, subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(SD_PATH, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(SD_PATH, subfolder=\"unet\")\n",
    "# unet = UNet2DModel.from_pretrained(SD_PATH, subfolder=\"unet\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(SD_PATH, subfolder=\"text_encoder\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(SD_PATH, subfolder=\"tokenizer\")\n",
    "# freeze parameters of models to save more memory\n",
    "unet.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "# Freeze the unet parameters before adding adapters\n",
    "for param in unet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "unet_lora_config = LoraConfig(\n",
    "    r=config[\"lora_rank\"],\n",
    "    lora_alpha=config[\"lora_rank\"],\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    ")\n",
    "\n",
    "# Move unet, vae to device and cast to weight_dtype\n",
    "weight_dtype = torch.float16\n",
    "unet.to(accelerator.device, dtype=weight_dtype)\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "# Add adapter and make sure the trainable params are in float32.\n",
    "unet.add_adapter(unet_lora_config)\n",
    "# only upcast trainable parameters (LoRA) into fp32\n",
    "cast_training_params(unet, dtype=torch.float32)\n",
    "\n",
    "lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
    "optimizer = torch.optim.AdamW(\n",
    "    lora_layers,\n",
    "    lr=1e-4,\n",
    "    # lr=3e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"constant\",\n",
    "    optimizer=optimizer,\n",
    "    # num_warmup_steps=500,\n",
    "    num_training_steps=config[\"train_epochs\"] * len(train_dataloader),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(pipeline, epoch):\n",
    "    nrows = 3\n",
    "    val_prompts = [\n",
    "        \"a man, in one piece style\",\n",
    "        \"a woman, in one piece style\",\n",
    "        \"an anime character, in one piece style\",\n",
    "    ] * nrows\n",
    "    images = pipeline(\n",
    "        # \"an image of an ocean landscape\",\n",
    "        val_prompts,\n",
    "        num_inference_steps=30,\n",
    "        generator=torch.Generator().manual_seed(config[\"seed\"]),\n",
    "    ).images  # List[PIL.Image]\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    # image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "    image_grid = make_image_grid(images, rows=nrows, cols=3)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config[\"output_dir\"], \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    SD_PATH,\n",
    "    unet=unet,\n",
    "    torch_dtype=weight_dtype,\n",
    ").to(accelerator.device)\n",
    "images = validate(pipeline, -1)\n",
    "del pipeline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for mixed precision training\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "# Train!\n",
    "global_step = 0\n",
    "train_loss = 0.0\n",
    "for epoch in range(config[\"train_epochs\"]):\n",
    "    progress_bar = tqdm(total=len(train_dataloader))\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "    unet.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Convert images to latent space\n",
    "        latents = vae.encode(batch[\"images\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise that we'll add to the latents\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps, (config[\"batch_size\"],), device=latents.device\n",
    "        )\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden_states = text_encoder(batch[\"captions\"], return_dict=False)[0].to(accelerator.device)\n",
    "        # Predict the noise residual and compute loss\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "\n",
    "        loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "        # Backpropagate\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "        accelerator.log(logs, step=global_step)\n",
    "        global_step += 1\n",
    "        train_loss = 0.0\n",
    "\n",
    "    if epoch % config[\"save_ckpt_every_n_epochs\"] == 0 or epoch == config[\"train_epochs\"] - 1:\n",
    "        save_path = os.path.join(config[\"output_dir\"], f\"epoch-{epoch:04d}\")\n",
    "        if os.path.exists(save_path):\n",
    "            shutil.rmtree(save_path)\n",
    "        # accelerator.save_state(save_path)\n",
    "\n",
    "        unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "        unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))\n",
    "\n",
    "        StableDiffusionPipeline.save_lora_weights(\n",
    "            save_directory=save_path,\n",
    "            unet_lora_layers=unet_lora_state_dict,\n",
    "            safe_serialization=True,\n",
    "        )\n",
    "\n",
    "        print(f\"Saved state to {save_path}\")\n",
    "\n",
    "    if epoch % config[\"validate_every_n_epochs\"] == 0 or epoch == config[\"train_epochs\"] - 1:\n",
    "        # create pipeline\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            # args.pretrained_model_name_or_path,\n",
    "            SD_PATH,\n",
    "            unet=accelerator.unwrap_model(unet),\n",
    "            # torch_dtype=weight_dtype,\n",
    "        ).to(accelerator.device)\n",
    "        images = validate(pipeline, epoch)\n",
    "\n",
    "        del pipeline\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "# Push to hub\n",
    "repo_id = create_repo(repo_id=\"kusnim1121/stable-diffusion-one-piece-lora\", exist_ok=True).repo_id\n",
    "model_description = f\"\"\"\n",
    "# LoRA text2image fine-tuning - {repo_id}\n",
    "These are LoRA adaption weights for {SD_PATH}. The weights were fine-tuned on the one-piece-with-caption dataset.\n",
    "\"\"\"\n",
    "model_card = load_or_create_model_card(\n",
    "    repo_id_or_path=repo_id,\n",
    "    from_training=True,\n",
    "    license=\"mit\",\n",
    "    base_model=SD_PATH,\n",
    "    model_description=model_description,\n",
    "    inference=True,\n",
    ")\n",
    "\n",
    "tags = [\n",
    "    \"stable-diffusion\",\n",
    "    \"stable-diffusion-diffusers\",\n",
    "    \"text-to-image\",\n",
    "    \"diffusers\",\n",
    "    \"diffusers-training\",\n",
    "    \"lora\",\n",
    "]\n",
    "model_card = populate_model_card(model_card, tags=tags)\n",
    "model_card.push_to_hub(repo_id)\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=os.path.join(config[\"output_dir\"], \"epoch-0000\"),\n",
    "    commit_message=\"End of training\",\n",
    "    # ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    SD_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "# load attention processors\n",
    "pipeline.load_lora_weights(os.path.join(\"./output\", \"epoch-0499\"))\n",
    "images = pipeline(\n",
    "    prompt=\"an anime character, one piece style\",\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(config[\"seed\"]),\n",
    "    num_inference_steps=30,\n",
    ").images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.unload_lora_weights()\n",
    "images = pipeline(  # TODO: fix\n",
    "    prompt=\"an anime character, one piece style\",\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(config[\"seed\"]),\n",
    "    num_inference_steps=30,\n",
    ").images\n",
    "images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
